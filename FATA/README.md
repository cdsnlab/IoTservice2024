# Feature Augmentation based Test-time Adaptation (FATA)

ê¸°ì¡´ì˜ í…ŒìŠ¤íŠ¸ ì‹œì  ì ì‘ (Test-Time Adaptation, TTA) ë°©ë²•ë“¤ì´ ì ì‘ì— ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ë°ì´í„°ê°€ ì œí•œë˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, Feature Augmentation based Test-time Adaptation (FATA) ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ë‹¤. 
FATAëŠ” ë°ì´í„° ìƒ˜í”Œì´ ì ì€ ìƒí™©ì—ì„œë„ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ìœ ì§€í•˜ê³ , ë„ë©”ì¸ ë³€í™”ì— íš¨ìœ¨ì ìœ¼ë¡œ ì ì‘í•  ìˆ˜ ìžˆë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤.

## Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors

This implementation is build on [Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors ðŸ”—](https://openreview.net/forum?id=9w3iw8wDuE) 
by Jonghyun Lee, Dahuin Jung, Saehyung Lee, Junsung Park, Juhyeon Shin, Uiwon Hwang and Sungroh Yoon (**ICLR 2024 Spotlight, Top-5% of the submissions**).

## Environments  

You should modify [username] and [env_name] in environment.yaml, then  
> $ conda env create --file environment.yaml  

## Dataset
You can download ImageNet-C from a link [ImageNet-C ðŸ”—](https://zenodo.org/record/2235448).  

After downloading the dataset, move to the root directory ([data_root]) of datasets.  

If you run on [ColoredMNIST ðŸ”—](https://arxiv.org/abs/1907.02893) or [Waterbirds ðŸ”—](https://arxiv.org/abs/1911.08731), run  
> $ python pretrain_[dataset_name].py --root_dir [data_root] --dset [dataset_name]

Then datasets are automatically downloaded in your [data_root] directory.  
(ColoredMNIST from [torchvision ðŸ”—](https://pytorch.org/vision/stable/index.html) and ./dataset/ColoredMNIST_dataset.py, Waterbirds from [wilds ðŸ”—](https://pypi.org/project/wilds/) package)

Your [data_root] will be as follows:
```bash
data_root
â”œâ”€â”€ ImageNet-C
â”‚   â”œâ”€â”€ brightness
â”‚   â”œâ”€â”€ contrast
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ColoredMNIST
â”‚   â”œâ”€â”€ ColoredMNIST_model.pickle
â”‚   â”œâ”€â”€ MNIST
â”‚   â”œâ”€â”€ train1.pt
â”‚   â”œâ”€â”€ train2.pt
â”‚   â””â”€â”€ test.pt
â”œâ”€â”€ Waterbirds
â”‚   â”œâ”€â”€ metadata.csv
â”‚   â”œâ”€â”€ waterbirds_dataset.h5py
â”‚   â”œâ”€â”€ waterbirds_pretrained_model.pickle
â”‚   â”œâ”€â”€ 001. Black_footed_Albatross
â”‚   â”œâ”€â”€ 002. Laysan_Albatross
â””â”€â”€ â””â”€â”€ ...
```
If you don't want to pre-train, you can just copy and paste the [dataset_name]_model.pickle from './pretrained/' directory.

## Experiment

You can run most of the experiments in our paper by  
> $ chmod +x exp_deyo.sh  
> $ ./exp_deyo.sh  

If you want to run on the ImageNet-R or VISDA-2021, you should use main_da.py

You should modify ROOT variable as [data_root] in exp_deyo.sh.  
